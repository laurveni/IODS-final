---
title: "Human development data"
author: "Laura Venieri, email: laura.venieri@helsinki.fi"
date: April 8, 2018
output: 
  html_document:
    theme: flatly
    toc: true
    toc_depth: 2
    fig_caption: true
    fig_width: 8
    fig_height: 6
    code_folding: hide
---

## Abstract

We analyse data related to the development of a country and investigate the relation between the Gross National Income (GNI) per capita, the life expectancy and the expected years of education. A high GNI per capita corresponds to high life expectancy and a longer education.

## Research questions

What can we say about the relation between the Gross National Income (GNI) per capita of a country and the life expectancy of its people? It seems quite natural to believe that a lower GNI per capita would correspond to a lower life expectancy. 

What about the relation between the GNI per capita and the expected years of education? Also in this case, one expects that people living in countries with low GNI have fewer years of education.

We will investigate whether these relationships are linear and whether we can say something else about them.

## Data

The 'human' dataset originates from the United Nations Development Programme. For more information see [their data page](http://hdr.undp.org/en/content/human-development-index-hdi) and the [technical notes](http://hdr.undp.org/sites/default/files/hdr2015_technical_notes.pdf) about calculating the human development and gender inequality indices.

We start with two data sets: [one](http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv) containing data about human development in various countries, [the other](http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv) about gender inequality.

The data wrangling was done in [this script](https://github.com/laurveni/IODS-final/blob/master/data_wrangling_human.R). The two original datasets have the following structure. The 'Human Development' dataset contains 195 observations of 8 variables:

* Human Development Index (HDI) rank
* Country
* HDI
* Life expectancy at Birth
* Expected Years of Education
* Mean Years of Education
* Gross National Income (GNI) per capita
* GNI per capita rank minus HDI rank

The 'Gender Inequality' dataset contains 195 observations of 10 variables:

* Gender Inequality Index (GII) rank
* Country
* GII
* Maternal Mortality Ratio
* Adolescent Birth Rate
* Percent of Female Representation in Parliament
* Percentage of Female Population with Secondary Education
* Percentage of Male Population with Secondary Education
* Female participation in Labour Force
* Male participation in Labour Force

After shortening the variable names, we computed two new variables: the ratio of Female and Male populations with secondary education and the ratio of labour force participation of females and males. We then joind the two datasets, using country as identifier, into the 'human' dataset, containing now 195 observations of 19 variables. We keep only the following columns:

* country
* sedu_ratio = Ratio of females/males with at least secondary education
* lab_ratio = Ratio of females/males in the labour force
* life_exp = Life expectancy at birth
* exp_edu = Expected years of schooling 
* gni_c = Gross National Income per capita
* mat_mort = Maternal mortality ratio
* ad_birth = Adolescent birth rate
* repr_parl = Percetange of female representatives in parliament

We filtered out all rows with missing values (NA) and we deleted the last 7 rows since they do not contain countries but continents or group of countries. Finally, we added the countries as row names and deleted the column "country".

Let us look at the structure of our final data set, which now contains 155 obsservations of  8 variables.

```{r}
human_ <- read.csv(file="human.csv")
str(human_)
```


We now visualize an overview of the data and the correlation matrix to get some insight on the relationships between the variables.

```{r message=FALSE}
library(GGally)
library(dplyr)
library(corrplot)
# visualize the 'human_' variables
ggpairs(human_)

# compute the correlation matrix and visualize it with corrplot
cor(human_) %>% corrplot
```

We can observe that life expectancy (life_exp) and maternal mortality ratio (mat_mort) are negatively correlated: it is not surprising since women give birth when they are relatively young, so if a high percentage of them dies the life expectancy will be lower. Maternal mortality ratio has also quite strong negative correlation with expected years of education (exp_edu) and ratio of females/males with at least secondary education (sedu_ratio).

Note that there is a strong positive correlation between life expectancy and expected years of education. Moeover, as we had predicted, they both have a quite strong positive correlation with GNI per capita (gni_c). We will now investigate more these relations, analyzing whether they are linear and how we can group our observations into clusters.

## Methods

To analyse the data we will use two methods:

* Linear regression to determine whether the relations between GNI per capita, life expectancy and expected years of education are linear;

* K-means to find patterns in the data.

**First method: Linear regression**

*Description of the method*

Given a certain number of variables, a regression model assumes that the relationship between the dependent variable $y$ (called target) and the explanatory variables is linear. Since we will use two explanatory variables, we explain this case and call them $x,z$. The model takes the form
$$y= ax+bz+c+e,$$
where $a$ and $b$ are the coefficients, $c$ is the intercept and $e$ is the error term or noise, an unobservable random variable. The goal is to estimate the parameters $a$, $b$ and $c$. The best model is found by minimizing the prediction errors, called residuals, that are the differences between the actual values $y$ of the target variable and the predicted values $\hat{y}$. In particular, the fitted model is the one minimizing the sum of squared residuals
$$ \sum_{i=1}^n (y_i-\hat{y}_i)^2, $$
where $n$ is the number of observations.

The assumptions of the method, a part from the linearity of the relation between the target and the explanatory variables, are the following:

* The errors are normally distributed
* The errors are not correlated
* The errors have constant variance
* The size of a given error does not depend on the explanatory variables.


*Fitting a linear regression model to our data*

We now fit a regression model with gni_c as the target and life_exp and exp_edu as the explanatory variables. Here is a summary of the model.

```{r message=FALSE}
#linear regression model
my_model <- lm(gni_c ~ life_exp + exp_edu, data = human_)
summary(my_model)
```

The p-values of both explanatory variables are very low (much less than $0.05$) so they have a statistically significant relation with the Gross National Income per capita. We can thus reject the null hypothesis that the coefficients equal zero. 

The multiple R-squared of the model is, however, 43.7%, which is quite low. This does not mean that the model is not significant, but it explains little of the variablity so it would not produce precise predictions. Moreover, the standard errors of both estimated coefficients are quite high since they are only roughly 3.5 times less than the coefficients estimates, whereas ideally they should be at least $10$ times less. 

The p-value of the F-test is very small (less than $0.05$) so a model with fewer explanatory variables would not be better.

Let us now look at the diagnostic plots to investigate if the hypotheses of the model are reasonable.

```{r message=FALSE}
par(mfrow = c(2,2))
plot(my_model, which=c(1,2,5))
```

The Residuals vs Fitted plot shows the error residuals vs the fitted values. There is not a distinctive pattern in the residuals, even if the points are not very scattered so the size of the errors may depend on the explanatory variables. This means that the assumption that the variance of the errors is constant might not be reasonable. There are also a few outliers (observations 30, 44 and 11), that is points with much bigger residuals than the others. 

In the Normal QQ-plot we can observe that there is a quite good fit to the line so the assumption that the errors of the model are normally distributed is reasonable. The points standing out from the line are again the observations 30, 44 and 11.

The Residuals vs Leverage plot shows that all cases are inside the Cook's distance lines, so there are no observations with unusually high impact.

**Second method: K-means**

*Description of the method*

K-means is an unsupervised learning algorithm that aims to partition the given observations into K clusters based on their similarities. Unsupervised means that it only tries to find patterns in the data without making predictions.

One first chooses the number of clusters, then the algorithm picks initial cluster centroids randomly and assigns each observation to the cluster whose centroid is the closest. It then repeats the following steps:
* calculate the new centroid of each cluster
* reassign each observation to the closest cluster
until certain stopping conditions are satisfied, such as the centroids or the clusters do not change or the within cluster variation cannot be reduced any further.

Using different distances one can obtain different results, here we will use the Euclidean distance. There are different ways to determine the optimal number of clusters, one is looking at the total within cluster sum of squares, which is the sum over every cluster of the following quantity:
$$WCSS=\sum_{i=1}^m |x_i- c|^2,$$
where $m$ is the number of observations in the cluster and $c$ is the centorid.

Looking at the plot of the number of clusters and the total within cluster sum of squares, the optimal number of clusters is that corresponding to a quick drop in the total WCSS.

*Applying K-means to our data*

We will apply the K-means algorithm to the columns gni_c, life_exp and exp_edu of our dataset to see if the patterns found with this method correspond to our hypotheses.

We first select the three desired columns and work only with those. We standardize them, scaling the variables to get comparable distances. We display a summary of the Euclidean distances between the observations. 

```{r}
#scale the variables
human_ <- select(human_, one_of(c("gni_c", "life_exp", "exp_edu")))
human_s <- scale(human_)
human_s <- as.data.frame(human_s)
# euclidean distance matrix
dist_eu <- dist(human_s)
# look at the summary of the distances
summary(dist_eu)
```


To investigate what is the optimal number of clusters, we look at how the total of within cluster sum of squares (tWCSS) behaves when the number of clusters changes. Looking at the following plot two clusters seem to be optimal.


```{r}
library(ggplot2)
set.seed(123)

# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
tWCSS <- sapply(1:k_max, function(k){kmeans(human_s, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = tWCSS, geom = 'line')
```

We now run the K-means algorithm with two clusters and visualize the clusters in the same variables.

```{r}
library(GGally)
# k-means clustering with 2 clusters
km <-kmeans(human_s, centers = 2)
# plot the boston_s dataset with clusters
pairs(human_s,col = km$cluster)
```

The groups determined by the algorithm correspond in some way to our hypetheses: those countries with lower GNI per capita have in most cases also lower life expectactancy and expected years of education. There are, however, some exceptions so this trend is not so clear.

## Conclusions

There is certainly a positive correlation between the Gross National Income per capita and the life expectancy, as well as between the GNI per capita and the expected years of education. Countries with higher GNI tend to have also higher life expectancy and more expected years of education. This is confirmed both by the K-means algorithm and the linear regression model. The results obtained with the linear regression model are, however, not so clear. There is a significant linear relation between the variables but it does not seem to explain enough of the variability. One option for further investigation could be to try to fit a regression model with a higher number of explanatory variables, for example including the maternal mortality ratio and the ratio of females/males with at least secondary education.