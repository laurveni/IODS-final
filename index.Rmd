---
title: "An analysis of human development data: how is the wealth of a country related to the life expectancy and the expected years of schooling of its population?"
author: "Laura Venieri, email: laura.venieri@helsinki.fi"
date: April 17, 2018
output: 
  html_document:
    theme: flatly
    toc: true
    toc_depth: 2
    fig_caption: true
    fig_width: 8
    fig_height: 6
    code_folding: hide
---

## Abstract

We analyse data related to the development of various world countries and investigate the relations between Gross National Income (GNI) per capita, life expectancy and expected years of schooling. Using linear regression, we can model the GNI as a linear function of life expectancy and expected years of education, but this model explains only roughly 43% of the variability. Using the K-means algorithm, we confirm that countries with high GNI per capita tend to have also high life expectancy and more expected years of schooling. 

## Research questions

What can we say about the relation between the Gross National Income (GNI) per capita of a country and the life expectancy of its population? It seems quite natural to believe that richer countries would have higher life expectancy than poor ones. 

What about the relation between GNI per capita and expected years of schooling? Also in this case, one expects that people living in countries with high GNI have higher education on average.

We will investigate whether we can fit a statistically significant linear model that predicts the GNI of a country based on the life expectancy and expected years of education of its population. We will also try to find patterns in the data to check whether they confirm our hypotheses.

## Data

We will combine two datasets that originate from the United Nations Development Programme. For more information see [their data page](http://hdr.undp.org/en/content/human-development-index-hdi) and the [technical notes](http://hdr.undp.org/sites/default/files/hdr2015_technical_notes.pdf) about calculating the human development and gender inequality indices.

[One](http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv) of the datasets contains data about human development in various countries, [the other](http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv) about gender inequality. They have the following structure. The 'Human Development' dataset contains 195 observations of 8 variables:

* Human Development Index (HDI) rank;
* Country;
* HDI;
* Life expectancy at birth;
* Expected years of education;
* Mean years of education;
* Gross National Income (GNI) per capita;
* GNI per capita rank minus HDI rank.

The 'Gender Inequality' dataset contains 195 observations of 10 variables:

* Gender Inequality Index (GII) rank;
* Country;
* GII;
* Maternal mortality ratio;
* Adolescent birth rate;
* Percentage of female representation in Parliament;
* Percentage of female population with secondary education;
* Percentage of male population with secondary education;
* Female participation in labour force;
* Male participation in labour force.

The data wrangling was done in [this script](https://github.com/laurveni/IODS-final/blob/master/data_wrangling_human.R). After shortening the variable names, we computed two new variables: the ratio of female and male population with secondary education and the ratio of labour force participation of females and males. We then joined the two datasets, using country as identifier, into the 'human' dataset, containing now 195 observations of 19 variables. We then kept only the following columns:

* country;
* sedu_ratio = Ratio of females/males with at least secondary education;
* lab_ratio = Ratio of females/males in the labour force;
* life_exp = Life expectancy at birth;
* exp_edu = Expected years of schooling; 
* gni_c = Gross National Income per capita;
* mat_mort = Maternal mortality ratio;
* ad_birth = Adolescent birth rate;
* repr_parl = Percentage of female representatives in Parliament.

We filtered out all rows with missing values (NA) and we deleted the last 7 rows since they did not contain countries but continents or group of countries. Finally, we added the countries as row names and deleted the column "country".

Let us look at the structure of our final dataset, which now contains 155 obsservations of 8 variables.

```{r}
human_ <- read.csv(file="human.csv")
str(human_)
```


We now visualize an overview of the data and the correlation matrix to get some insights into the relations between the variables.

```{r message=FALSE}
library(GGally)
library(dplyr)
library(corrplot)
# visualize the 'human_' variables
ggpairs(human_)

# compute the correlation matrix and visualize it with corrplot
cor(human_) %>% corrplot
```

We can observe that life expectancy (life_exp) and maternal mortality ratio (mat_mort) are negatively correlated: it is not surprising since women have children when they are relatively young, so if a high percentage of them dies when giving birth the life expectancy will be lower. Maternal mortality ratio has also quite strong negative correlation with expected years of education (exp_edu) and ratio of females/males with at least secondary education (sedu_ratio).

Note that there is a strong positive correlation between life expectancy and expected years of education. Moreover, as we had expected, they both have a quite strong positive correlation with GNI per capita (gni_c). We will now investigate more these relations, analyzing whether they are linear and how we can group our observations into clusters.

## Methods and Analysis

To analyse the data we will use two methods:

* Linear regression to determine whether the relations between GNI per capita, life expectancy and expected years of education are close to linear;

* K-means to find patterns in the data.

**First method: Linear regression**

*Description of the method*

Linear regression can help us understand the relationship between a certain variable $y$, called target, and one or more explanatory variables, also called predictors. Since we will use a model with two explanatory variables, we explain this case and call them $x,z$. The model takes the form
$$y_i= ax_i+bz_i+c+\epsilon_i, \quad  i=1, \dots, n,$$
where $n$ is the number of observations in the dataset.
The parameters $a$ and $b$ are the coefficients, $c$ is the intercept, which is the predicted value when $x=z=0$ and $\epsilon_i$ are the error terms, which are assumed to be unobservable random variables. The goal is to estimate the parameters $a$, $b$ and $c$. The best model is found by minimizing the prediction errors, called residuals, that are the differences between the actual values $y$ of the target variable and the predicted values $\hat{y}$. In particular, the fitted model is the one minimizing the sum of squared residuals
$$ \sum_{i=1}^n (y_i-\hat{y}_i)^2. $$


The assumptions of the method, apart from the linearity of the relation between the target and the explanatory variables, are the following:

* the errors are normally distributed;
* the errors are not correlated;
* the errors have constant variance;
* the size of a given error does not depend on the explanatory variables.


*Fitting a linear regression model to our data*

We now fit a regression model with gni_c as the target and life_exp and exp_edu as the explanatory variables. Here is a summary of the model.

```{r message=FALSE}
#linear regression model
my_model <- lm(gni_c ~ life_exp + exp_edu, data = human_)
summary(my_model)
```

The p-values of both explanatory variables are very low (roughly $0.0005$ and $0.0007$) so both variables have a statistically significant relation with the Gross National Income per capita. We can thus reject the null hypothesis that the coefficients equal zero. 

The multiple R-squared of the model is, however, 43.7%, which is quite low. This does not mean that the model is not significant, but it explains little of the variablity so it would not produce precise predictions. Moreover, the standard errors of both estimated coefficients are quite high since they are only roughly 3.5 times less than the coefficients estimates, whereas ideally they should be at least $10$ times less. 

The p-value of the F-test is very small (less than $0.00001$) so a model with fewer explanatory variables would not be better.

Let us now look at the diagnostic plots to investigate if the assumptions of the model are reasonable.

```{r message=FALSE}
par(mfrow = c(2,2))
plot(my_model, which=c(1,2,5))
```

The Residuals vs Fitted plot shows the error residuals vs the fitted values. There is not a distinctive pattern in the residuals, even if the points are not very scattered so the size of the errors may depend on the explanatory variables. This means that the assumption that the variance of the errors is constant might not be reasonable. There are also a few outliers (observations 30, 44 and 11), that is points with much bigger residuals than the others. 

In the Normal QQ-plot we can observe that there is a quite good fit to the line so the assumption that the errors of the model are normally distributed seems reasonable. The points standing out from the line are again the observations 30, 44 and 11.

The Residuals vs Leverage plot shows that all cases are inside the Cook's distance lines, so there are no observations with unusually high impact.

**Second method: K-means**

*Description of the method*

K-means is an unsupervised learning algorithm that aims to partition the given observations into K clusters based on their similarities. Unsupervised means that it only tries to find patterns in the data without making predictions.

One first chooses the number of clusters, then the algorithm picks initial cluster centroids randomly and assigns each observation to the cluster whose centroid is the closest. It then repeats the following steps:

* calculate the new centroid of each cluster
* reassign each observation to the cluster whose centroid is the closest

until certain stopping conditions are satisfied, such as the centroids or the clusters do not change or the within cluster variation cannot be reduced any further.

Using different distances one can obtain different results, here we will use the Euclidean distance. There are different ways to determine the optimal number of clusters, one is looking at the total within cluster sum of squares, which is the sum over every cluster of the following quantity:
$$WCSS=\sum_{i=1}^m |x_i- c|^2,$$
where $m$ is the number of observations in the cluster and $c$ is the centorid.

Looking at the plot of the number of clusters and the total within cluster sum of squares, the optimal number of clusters is that corresponding to a quick drop in the total WCSS.

*Applying K-means to our data*

We will apply the K-means algorithm to the variables gni_c, life_exp and exp_edu of our dataset to see if the patterns found with this method correspond to our hypotheses.

We first select the three desired columns and work only with those. We standardize them, scaling the variables to get comparable distances. We display a summary of the Euclidean distances between the observations. 

```{r}
#select the columns gni_c, life_exp and exp_edu
human_ <- select(human_, one_of(c("gni_c", "life_exp", "exp_edu")))
#scale the variables
human_s <- scale(human_)
human_s <- as.data.frame(human_s)
# euclidean distance matrix
dist_eu <- dist(human_s)
# look at the summary of the distances
summary(dist_eu)
```


To investigate what is the optimal number of clusters, we look at how the total within cluster sum of squares (tWCSS) behaves when the number of clusters changes. Looking at the following plot two clusters seem to be optimal.


```{r}
library(ggplot2)
set.seed(123)

# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
tWCSS <- sapply(1:k_max, function(k){kmeans(human_s, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = tWCSS, geom = 'line')
```

We now run the K-means algorithm with two clusters and visualize the clusters in the same variables.

```{r}
library(GGally)
# k-means clustering with 2 clusters
km <-kmeans(human_s, centers = 2)
# plot the variables with clusters
pairs(human_s,col = km$cluster)
```

The groups determined by the algorithm correspond in some way to our hypetheses: those countries with lower GNI per capita have in most cases also lower life expectactancy and expected years of education. There are, however, some exceptions so this trend is not so clear.

## Conclusions

There is certainly a positive correlation between Gross National Income per capita and life expectancy, as well as between GNI per capita and expected years of education. Countries with higher GNI tend to have also higher life expectancy and more expected years of education. This is confirmed both by the K-means algorithm and the linear regression model. The results obtained with the linear regression model are, however, not so clear. There is a significant linear relation between the variables but it does not seem to explain enough of the variability. One option for further investigation could be to try to fit a regression model with a higher number of explanatory variables, for example including the maternal mortality ratio and the ratio of females/males with at least secondary education.